# Deep learning

​	使用传统机器学习方法进行心律失常分类和诊断的各种研究在过去的几十年里已经发展了大量。这些研究旨在构建机器学习模型(如支持向量机[17]、随机森林[18]、朴素贝叶斯、浅层神经网络，以帮助该领域的医生/专家获得更全面的分析和诊断。如Mohebbi等人[20]提出了一种基于线性判别分析的特征约简方法，并将提取的特征输入支持向量机进行心电图异常模式的识别。然而，传统的机器学习方法也面临着一些不容忽视的问题。首先，原始心电数据的预处理复杂、混乱，费时费力，但是否对最终结果有积极影响尚不确定。此外，特征工程(如小波分解和时域/频域特征提取)也需要大量的时间。人工提取的特征可能无助于获得最优的分类结果。

建立在深度神经网络之上的分类器也是一个研究热点[2]，[21]-[24]。例如，Kiranyaz等人[25]开发了一种针对患者的心电分类和监测系统，对于每个单个用户，只有相对较小的公共和个人特定数据可以输入到一维CNN中。将特征提取和分类过程结合到一个单一的学习框架中，将进一步提高分类系统的性能。在[26]中，Acharya等人提出了一种基于CNN的计算机辅助诊断系统，可以自动识别不同的心电信号段。采用的CNN包含11层，然后是输出层，输出层有4个节点，每一个节点都描述了一种反复出现的危及生命的心律失常。在实验中，采用了去掉QRS复波检测后的2秒和5秒的心电信号。

From an architecture point of view, any single convolution can be replaced by a Dense layer that would perform the same association of neighboring pixels for each pixel. It would mean one neuron per pixel with not-null coefficients only on the neighbors. Convolutional layer is enforcing parameter sharing: the processing of each pixel is identical by design, not by learning. It means a dramatic reduction in the number of parameters to learn, still with a very good performance, as we will see in the evaluation section.(单个卷积层完全可以等价的被密集层代替。卷积层相当于一个每个像素仅仅与相邻元素具有相关系数的密集层。这使得像素信息的处理被人为的规定了，而不是完全依靠学习得到，大大减少了所需要学习的参数，并且依然有很好的表现)

##  Embedding 

[Embedding 的理解](https://zhuanlan.zhihu.com/p/46016518)

## Regularization

Regularization is a set of techniques to speed up the convergence during training and to avoid overfitting. There are a few families of regularization:

· **Penalization**: a penalization term is added to the gradient back-propagation in order to “pushback” the coefficients toward 0. The classical penalizations are the Lasso (based on L1 norm) and Ridge (based on L2 normed), there are many others which are varying the used norm, and there is the Elastic-Net [7] that is combining Lasso and Ridge.

· **Early stopping**: given that overfitting is happening when the network is learning the training samples, and that it is observable through a performance drop during validation, the training is stopped when such inflexion is detected.

· **Dropout** [6]: for each batch, a random portion of the outputs are nullified in order to avoid strong dependencies between portions of adjacent layers. This technique is similar to boosting techniques on decision trees.

· **Data augmentation**: more training samples are created from existing one through geometrical transformations (translation, scaling, rotation…) or filters (blur).

· **Model size reduction** to tilt the ratio number of coefficients over number of training samples.

## Transform learning

source data train model, target data fine-tuning

- Conservative learning
- Layer training
- progressive neural net work
- domain-adversarial training
- Zero-Shot learning

## Recurrent Neural Network

Long short-term Memory

# Ensemble learning

**Ensemble Learning** performs a strategic combination of various experts or ML models in order to improve the effectiveness obtained using a single weak model 

## The meaning of Ensemble

A low bias and a low variance, although they most often vary in opposite directions, are the two most fundamental features expected for a model. Indeed, to be able to “solve” a problem, we want our model to have enough degrees of freedom to resolve the underlying complexity of the data we are working with, but we also want it to have not too much degrees of freedom to avoid high variance and be more robust. This is the well known **bias-variance tradeoff**.

![img](D:\project\笔记\毕设\图片\bia&var.png)

In ensemble learning theory, we call **weak learners** (or **base models**) models that can be used as building blocks for designing more complex models by combining several of them. Most of the time, these basics models perform not so well by themselves either because they have a high bias (low degree of freedom models, for example) or because they have too much variance to be robust (high degree of freedom models, for example). Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several of them together in order to create a **strong learner** (or **ensemble model**) that achieves better performances.

## Combine weak learners

In order to set up an ensemble learning method, we first need to select our base models to be aggregated. Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have homogeneous weak learners that are trained in different ways. The ensemble model we obtain is then said to be “homogeneous”. However, there also exist some methods that use different type of base learning algorithms: some heterogeneous weak learners are then combined into an “heterogeneous ensembles model”.

![img](D:\project\笔记\毕设\图片\ensembleweak.png)

There are different types of **Ensemble Learning** techniques which differ mainly by the type of models used (*homogeneous* or *heterogeneous models*), the data sampling (*with or without replacement*, *k-fold*, etc.) and the decision function (*voting*, *average*, *meta model*, etc). Therefore, **Ensemble Learning** techniques can be classified as:

- **Bagging**, that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process
- **Boosting**, that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy
- **Stacking**, that often considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions

In addition to these three main categories, two important variations emerge: **Voting** (which is a complement of *Bagging*) and **Blending** (a subtype of *Stacking*). Although **Voting** and **Blending** are a complement and a subtype of **Bagging** and **Stacking** respectively, these techniques are often found as direct types of **Ensemble Learning**.

Very roughly, we can say that bagging will mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).

## Bagging

![img](D:\project\笔记\毕设\图片\bagging.png)

The **random forest** approach is a bagging method where **deep trees**, fitted on bootstrap samples, are combined to produce an output with lower variance. However, random forests also use another trick to make the multiple fitted trees a bit less correlated with each others: when growing each tree, instead of only sampling over the observations in the dataset to generate a bootstrap sample, we also **sample over features** and keep only a random subset of them to build the tree.

![img](D:\project\笔记\毕设\图片\randomForest.png)

## Boosting

### Adaptive Boosting

In adaptative boosting (often called “adaboost”), we try to define our ensemble model as a weighted sum of L weak learners

### Gradient Boosting

## Stacking

The **Stacking Generalization** method is commonly composed of 2 training stages, better known as “*level 0*” and “*level 1*”. It is important to mention that it can be added as many levels as necessary. However, in practice it is common to use only 2 levels. The aim of the first stage (*level 0*) is to generate the training data for the *meta-model*, this is carried out by implementing k-fold cross validation for each “*weak learner*” defined in the first stage. The predictions of each one of these“*weak learners*” are “*stacked*” in order to build such such “*new training set*” (the *meta-model*). The aim of the second stage (*level 1*) is to train the meta-model, such training is carried out through an already determined “*final learner*”.

![img](D:\project\笔记\毕设\图片\stacking.jpeg)

## Blending

**Blending** is a technique derived from **Stacking Generalization**. The only difference is that in **Blending**, the *k-fold cross validation* technique is not used to generate the training data of the *meta-model.* Blending implements “*one-holdout set*”, that is, a small portion of the training data (*validation*) to make predictions which will be “*stacked*” to form the training data of the *meta-model*. Also, predictions are made from the test data to form the *meta-model* test data.

![img](D:\project\笔记\毕设\图片\blending.jpeg)

## Voting

he architecture of a **Voting Classifier** is made up of a number “*n*” of ML models, whose predictions are valued in two different ways: **hard** and **soft**. In **hard mode**, the winning prediction is the one with “*the most votes*”.On the other hand, the **Voting Classifier** in **soft mode** considers the probabilities thrown by each ML model, these probabilities will be *weighted* and *averaged*, consequently the *winning class* will be the one with the *highest weighted* and *averaged* probability.

# Ensemble  of DNN
